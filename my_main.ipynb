{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "from statistics import mode\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "model_path = \"/workspace/models/InternVL-Chat-V1-5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# 4bit量子化の設定\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 学習済みモデルの読み込み\n",
    "intern_model = AutoModel.from_pretrained(model_path, quantization_config=bnb_config, device_map=\"auto\", torch_dtype=torch.bfloat16).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = text.lower()\n",
    "    num_word_to_digit = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
    "        'ten': '10'\n",
    "    }\n",
    "    for word, digit in num_word_to_digit.items():\n",
    "        text = text.replace(word, digit)\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "    contractions = {\n",
    "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
    "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
    "    }\n",
    "    for contraction, correct in contractions.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_path, image_dir, model, tokenizer, answer=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.df = pd.read_json(df_path)\n",
    "        self.answer = answer\n",
    "\n",
    "        self.answer2idx = {}\n",
    "        self.idx2answer = {}\n",
    "\n",
    "        if self.answer:\n",
    "\n",
    "            # Training_dataに含まれるAnswerを全て取得\n",
    "            for answers in self.df[\"answers\"]:\n",
    "                for answer in answers:\n",
    "                    word = answer[\"answer\"]\n",
    "                    word = process_text(word)\n",
    "                    if word not in self.answer2idx:\n",
    "                        self.answer2idx[word] = len(self.answer2idx)\n",
    "            # 追加でClass_mappingに含まれるAnswerを取得\n",
    "            class_mapping = pd.read_csv(\"/workspace/class_mapping.csv\")\n",
    "            self.idx2answer = {}\n",
    "            for word, idx in zip(class_mapping[\"answer\"], class_mapping[\"class_id\"]):\n",
    "                word = process_text(word)\n",
    "                self.answer2idx[word] = idx\n",
    "\n",
    "            self.idx2answer = {v: k for k, v in self.answer2idx.items()}\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def update_dict(self, dataset):\n",
    "        self.answer2idx = dataset.answer2idx\n",
    "        self.idx2answer = dataset.idx2answer\n",
    "\n",
    "    def extract_text_features(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=303)\n",
    "        input_ids = inputs.input_ids.to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.language_model.model.tok_embeddings(input_ids)\n",
    "        return text_features\n",
    "\n",
    "    def extract_image_features(self, pixel_values):\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.vision_model.embeddings(pixel_values)\n",
    "        return image_features\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n",
    "        input_size = 224\n",
    "        max_num = 1\n",
    "        transform = build_transform(input_size=input_size)\n",
    "        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(image) for image in images]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        pixel_values = pixel_values.to(torch.bfloat16)\n",
    "        pixel_values = self.extract_image_features(pixel_values)\n",
    "\n",
    "        question = self.df[\"question\"][idx]\n",
    "        # 質問文の前処理\n",
    "        question = process_text(question)\n",
    "        question = self.extract_text_features(question)\n",
    "\n",
    "        if self.answer:\n",
    "            answers = [self.answer2idx[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\n",
    "            mode_answer_idx = mode(answers)\n",
    "            return pixel_values, question, torch.Tensor(answers), int(mode_answer_idx)\n",
    "        else:\n",
    "            return pixel_values, question\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):\n",
    "    total_acc = 0.\n",
    "    for pred, answers in zip(batch_pred, batch_answers):\n",
    "        acc = 0.\n",
    "        for i in range(len(answers)):\n",
    "            num_match = 0\n",
    "            for j in range(len(answers)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if pred == answers[j]:\n",
    "                    num_match += 1\n",
    "            acc += min(num_match / 3, 1)\n",
    "        total_acc += acc / 10\n",
    "    return total_acc / len(batch_pred)\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, n_answer: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # vision\n",
    "        self.vision_conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), stride=(3, 3), padding=1)\n",
    "        self.vision_bn1 = nn.BatchNorm2d(16)\n",
    "        self.vision_conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), stride=(3, 3), padding=1)\n",
    "        self.vision_bn2 = nn.BatchNorm2d(32)\n",
    "        self.vision_conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), stride=(3, 3), padding=1)\n",
    "        self.vision_bn3 = nn.BatchNorm2d(64)\n",
    "        self.vision_pool = nn.MaxPool2d(kernel_size=(3, 3), stride=(3, 3))\n",
    "\n",
    "        # text\n",
    "        self.text_conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(2, 2), stride=(2, 2), padding=1)\n",
    "        self.text_bn1 = nn.BatchNorm2d(4)\n",
    "        self.text_conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=(2, 2), stride=(2, 2), padding=1)\n",
    "        self.text_bn2 = nn.BatchNorm2d(8)\n",
    "        self.text_conv3 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(2, 2), stride=(2, 2), padding=1)\n",
    "        self.text_bn3 = nn.BatchNorm2d(16)\n",
    "        self.text_conv4 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(2, 2), stride=(2, 2), padding=1)\n",
    "        self.text_bn4 = nn.BatchNorm2d(32)\n",
    "        self.text_pool= nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        # combined\n",
    "        self.fc1 = nn.Linear(1600, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, n_answer)\n",
    "\n",
    "        # 重みの初期化\n",
    "        self._initialize_weights()\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, vision_input, text_input):\n",
    "        \n",
    "        # vision \n",
    "        v = F.relu(self.vision_bn1(self.vision_bn1(self.vision_conv1(vision_input))))\n",
    "        v = F.relu(self.vision_bn2(self.vision_bn2(self.vision_conv2(v))))\n",
    "        v = self.vision_pool(v)\n",
    "        v = F.relu(self.vision_bn3(self.vision_conv3(v)))\n",
    "        v = self.vision_pool(v)\n",
    "        v = v.view(v.size(0), -1)\n",
    "\n",
    "        # text\n",
    "        t = F.relu(self.text_bn1(self.text_conv1(text_input)))\n",
    "        t = self.text_pool(t)\n",
    "        t = F.relu(self.text_bn2(self.text_conv2(t)))\n",
    "        t = self.text_pool(t)\n",
    "        t = F.relu(self.text_bn3(self.text_conv3(t)))\n",
    "        t = self.text_pool(t)\n",
    "        t = F.relu(self.text_bn4(self.text_conv4(t)))\n",
    "        t = self.text_pool(t)\n",
    "        t = t.view(t.size(0), -1)\n",
    "\n",
    "        # combined\n",
    "        combined = torch.cat((v, t), dim=1)\n",
    "        x = F.relu(self.bn1(self.fc1(combined)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    simple_acc = 0\n",
    "    start = time.time()\n",
    "    for image, question, answers, mode_answer in tqdm(dataloader, desc=\"Batch\", leave=False):\n",
    "        image, question, answers, mode_answer = \\\n",
    "            image.to(device, dtype=torch.bfloat16), question.to(device, dtype=torch.bfloat16), answers.to(device), mode_answer.to(device)\n",
    "        pred = model(image, question)\n",
    "        loss = criterion(pred, mode_answer.squeeze().to(torch.long))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += VQA_criterion(pred.argmax(1), answers) \n",
    "        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item() \n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n",
    "\n",
    "def eval(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    simple_acc = 0\n",
    "    start = time.time()\n",
    "    for image, question, answers, mode_answer in dataloader:\n",
    "        image, question, answers, mode_answer = \\\n",
    "            image.to(device, dtype=torch.bfloat16), question.to(device, dtype=torch.bfloat16), answers.to(device), mode_answer.to(device)\n",
    "        pred = model(image, question)\n",
    "        loss = criterion(pred, mode_answer.squeeze().to(torch.long))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += VQA_criterion(pred.argmax(1), answers)  \n",
    "        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  \n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_dataset = VQADataset(df_path=\"/workspace/data/train.json\", image_dir=\"/workspace/data/train\", model=intern_model, tokenizer=tokenizer, answer=True)\n",
    "test_dataset = VQADataset(df_path=\"/workspace/data/valid.json\", image_dir=\"/workspace/data/valid\", model=intern_model, tokenizer=tokenizer, answer=False)\n",
    "test_dataset.update_dict(train_dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model = VQAModel(n_answer=len(train_dataset.answer2idx)).to(device)\n",
    "model = model.to(torch.bfloat16)  # モデル全体をbfloat16に変換\n",
    "\n",
    "\n",
    "num_epoch = 30\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "for epoch in tqdm(range(num_epoch), desc=\"Epoch\"):\n",
    "    train_loss, train_acc, train_simple_acc, train_time = train(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"【{epoch + 1}/{num_epoch}】\\n\"\n",
    "            f\"train time: {train_time:.2f} [s]\\n\"\n",
    "            f\"train loss: {train_loss:.4f}\\n\"\n",
    "            f\"train acc: {train_acc:.4f}\\n\"\n",
    "            f\"train simple acc: {train_simple_acc:.4f}\")\n",
    "    \n",
    "    if True:\n",
    "        model.eval()\n",
    "        submission = []\n",
    "        for image, question in test_loader:\n",
    "            image, question = image.to(device, dtype=torch.bfloat16), question.to(device, dtype=torch.bfloat16)\n",
    "            pred = model(image, question)\n",
    "            pred = pred.argmax(1).cpu().item()\n",
    "            submission.append(pred)\n",
    "\n",
    "        submission = [train_dataset.idx2answer[id] for id in submission]\n",
    "        submission = np.array(submission)\n",
    "        torch.save(model.state_dict(), f\"/workspace/submissions/model/model_{epoch}.pth\")\n",
    "        np.save(f\"/workspace/submissions/npy/submission_{epoch}.npy\", submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
