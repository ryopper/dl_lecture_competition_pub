InternVLChatModel(
  (vision_model): InternVisionModel(
    (embeddings): InternVisionEmbeddings(
      (patch_embedding): Conv2d(3, 3200, kernel_size=(14, 14), stride=(14, 14))
    )
    (encoder): InternVisionEncoder(
      (layers): ModuleList(
        (0): InternVisionEncoderLayer(
          (attn): InternAttention(
            (qkv): Linear4bit(in_features=3200, out_features=9600, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (q_norm): InternRMSNorm()
            (k_norm): InternRMSNorm()
            (inner_attn): FlashAttention()
            (proj): Linear4bit(in_features=3200, out_features=3200, bias=True)
          )
          (mlp): InternMLP(
            (act): GELUActivation()
            (fc1): Linear4bit(in_features=3200, out_features=12800, bias=True)
            (fc2): Linear4bit(in_features=12800, out_features=3200, bias=True)
          )
          (norm1): InternRMSNorm()
          (norm2): InternRMSNorm()
          (drop_path1): Identity()
          (drop_path2): Identity()
        )
        (1-44): 44 x InternVisionEncoderLayer(
          (attn): InternAttention(
            (qkv): Linear4bit(in_features=3200, out_features=9600, bias=False)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (q_norm): InternRMSNorm()
            (k_norm): InternRMSNorm()
            (inner_attn): FlashAttention()
            (proj): Linear4bit(in_features=3200, out_features=3200, bias=True)
          )
          (mlp): InternMLP(
            (act): GELUActivation()
            (fc1): Linear4bit(in_features=3200, out_features=12800, bias=True)
            (fc2): Linear4bit(in_features=12800, out_features=3200, bias=True)
          )
          (norm1): InternRMSNorm()
          (norm2): InternRMSNorm()
          (drop_path1): DropPath()
          (drop_path2): DropPath()
        )
      )
    )
  )
  (language_model): InternLM2ForCausalLM(
    (model): InternLM2Model(
      (tok_embeddings): Embedding(92553, 6144, padding_idx=2)
      (layers): ModuleList(
        (0-47): 48 x InternLM2DecoderLayer(
          (attention): InternLM2FlashAttention2(
            (wqkv): Linear4bit(in_features=6144, out_features=8192, bias=False)
            (wo): Linear4bit(in_features=6144, out_features=6144, bias=False)
            (rotary_emb): InternLM2DynamicNTKScalingRotaryEmbedding()
          )
          (feed_forward): InternLM2MLP(
            (w1): Linear4bit(in_features=6144, out_features=16384, bias=False)
            (w3): Linear4bit(in_features=6144, out_features=16384, bias=False)
            (w2): Linear4bit(in_features=16384, out_features=6144, bias=False)
            (act_fn): SiLUActivation()
          )
          (attention_norm): InternLM2RMSNorm()
          (ffn_norm): InternLM2RMSNorm()
        )
      )
      (norm): InternLM2RMSNorm()
    )
    (output): Linear4bit(in_features=6144, out_features=92553, bias=False)
  )
  (mlp1): Sequential(
    (0): LayerNorm((12800,), eps=1e-05, elementwise_affine=True)
    (1): Linear4bit(in_features=12800, out_features=6144, bias=True)
    (2): GELU(approximate='none')
    (3): Linear(in_features=6144, out_features=6144, bias=True)
  )
)